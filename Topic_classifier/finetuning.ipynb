{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c4c628-16cf-45b9-ac98-65b233b84792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 KB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.21.4 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d6249e1-73d7-40c8-ac23-80bb19a76890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8644b16-873e-4084-b2c0-e83ac1198b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2024.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.21.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.0.0\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (1.26.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 KB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 KB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.0\n",
      "    Uninstalling fsspec-2024.3.0:\n",
      "      Successfully uninstalled fsspec-2024.3.0\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 frozenlist-1.4.1 fsspec-2024.2.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 responses-0.18.0 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c81ebcc-5801-422a-bb68-098eab3bb3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0706a0eb-6aa7-4674-8356-d55a3b235d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('name-to-name.pickle', 'rb') as file:\n",
    "    name_to_name_dict = pickle.load(file)\n",
    "\n",
    "\n",
    "def replace_symbol(text):\n",
    "    text = re.sub(r'[\\d′‖√∠ω№◦⊥→×]', lambda match: name_to_name_dict[match.group()], text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def del_garbage(text):\n",
    "    return text.strip('.,)?.<*&^%$#@!~')\\\n",
    "               .replace(u'\\u00A0', ' ')\\\n",
    "               .replace(u'\\x0c', ' ')\\\n",
    "               .replace('\\n', ' ').replace('\\t', ' ')\\\n",
    "               .replace('\\r', ' ')\n",
    "\n",
    "\n",
    "# Убираем пунтктуацию\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "\n",
    "# Финальная функция обработки текста\n",
    "def preprocessing_text(text):\n",
    "    text = del_garbage(text).lower()\n",
    "    text = replace_symbol(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d712f554-7cb7-453b-ab05-66d5c351155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-19 19:11:46.447720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 19:11:47.072684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TrainingArguments\n",
    "import torch, os\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260cec13-4c36-4299-be04-f6760f7ff460",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('preprocessing_data_big.csv', sep='@')\n",
    "dataset = dataset[['task', 'topic']]\n",
    "dataset.rename(columns={'task': 'text',\n",
    "                        'topic': 'labels'},\n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58129296-a404-4eea-979e-c4fd2423f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = len(dataset.labels.unique())\n",
    "\n",
    "id2label = {id: label for id, label in enumerate(dataset.labels.unique())}\n",
    "\n",
    "label2id = {label: id for id, label in enumerate(dataset.labels.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7563c43f-ce39-4a8c-9ee1-da2bc53cdccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at blanchefort/rubert-base-cased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\n",
    "model = BertForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment',\n",
    "                                                           num_labels=NUM_LABELS, id2label=id2label,\n",
    "                                                           label2id=label2id,\n",
    "                                                     ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2db50ed8-8617-4ce5-8002-10fccd8d70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"y\"] = dataset.labels.map(lambda x: label2id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "795babcd-7b63-4bfb-822a-db0ba51e2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text_proc'] = dataset['text'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ef7fbb-2226-4385-8f3f-3869372c1a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'одинтри. (турнир городов, дванольодинпять, восемь–девять.четыре, одинноль–одинодин.два ) на кольцевой дороге через равные промежутки расположены двапять постов, на каждом стоит полицейский. полицейские пронумерованы в какомто порядке числами от один до двапять. требуется, чтобы они перешли по дороге так, чтобы снова на каждом посту был полицейский, но по часовой стрелке за номером один стоял номер два, за номером два стоял номер три, . . . , за номером двапять стоял номер один. докажите, что если организова'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['text_proc'].apply(lambda x: len(x)) > 512].reset_index().text_proc[3][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d0a6562-68f6-4d38-b649-661ad0b1e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text_proc'] = dataset['text_proc'].apply(lambda x: x[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45d5a905-fbd9-425c-98b2-172b03e9526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text_proc'], dataset['y'],\n",
    "                                                    test_size=0.4, random_state=42,\n",
    "                                                    )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99231924-77b2-4207-9aef-ad055928226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
    "val_encodings  = tokenizer(list(X_val), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36eb0bee-627d-40eb-ba6a-9700d1079eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve tokenized data for the given index\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Add the label for the given index to the item dictionary\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4fc49cb-7ae5-4d67-900d-575943439a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_encodings, list(y_train))\n",
    "\n",
    "val_dataloader = DataLoader(val_encodings, list(y_val))\n",
    "\n",
    "test_dataset = DataLoader(test_encodings, list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0deb4d23-66c1-4844-a056-ad93ad8f057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05cb0c4b-f6fd-4a22-b540-b537274d167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Extract true labels from the input object\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    # Obtain predicted class labels by finding the column index with the maximum probability\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    \n",
    "    # Calculate the accuracy score using sklearn's accuracy_score function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Return the computed metrics as a dictionary\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "004cdfcc-bcab-4ee6-8b73-5f2292f8b3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "571e56dc-22cd-4478-a575-f9d9ba0021b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written\n",
    "    output_dir='./TTC4900Model', \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #  The number of epochs, defaults to 3.0 \n",
    "    num_train_epochs=20,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=32,\n",
    "    # Number of steps used for a linear warmup\n",
    "    warmup_steps=100,                \n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "   # TensorBoard log directory                 \n",
    "    logging_dir='./multi-class-logs',            \n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\", \n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11bcbc86-d61f-4aab-861d-eb2c5539a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    # the pre-trained model that will be fine-tuned \n",
    "    model=model,\n",
    "     # training arguments that we defined above                        \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataloader,         \n",
    "    eval_dataset=val_dataloader,            \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4e7a69c-a3bb-47eb-b9e1-c532e522796d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 23:21, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.559600</td>\n",
       "      <td>1.561077</td>\n",
       "      <td>0.426357</td>\n",
       "      <td>0.129058</td>\n",
       "      <td>0.110152</td>\n",
       "      <td>0.176339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.551700</td>\n",
       "      <td>1.623814</td>\n",
       "      <td>0.428941</td>\n",
       "      <td>0.114711</td>\n",
       "      <td>0.092727</td>\n",
       "      <td>0.150973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.469200</td>\n",
       "      <td>1.725485</td>\n",
       "      <td>0.428941</td>\n",
       "      <td>0.111615</td>\n",
       "      <td>0.090278</td>\n",
       "      <td>0.147592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.605600</td>\n",
       "      <td>1.607271</td>\n",
       "      <td>0.435831</td>\n",
       "      <td>0.116384</td>\n",
       "      <td>0.093914</td>\n",
       "      <td>0.153326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.620400</td>\n",
       "      <td>1.824425</td>\n",
       "      <td>0.436693</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.090724</td>\n",
       "      <td>0.145896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.630100</td>\n",
       "      <td>1.714818</td>\n",
       "      <td>0.459087</td>\n",
       "      <td>0.152476</td>\n",
       "      <td>0.117040</td>\n",
       "      <td>0.220828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.589500</td>\n",
       "      <td>1.636785</td>\n",
       "      <td>0.422050</td>\n",
       "      <td>0.128727</td>\n",
       "      <td>0.111127</td>\n",
       "      <td>0.177213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.510500</td>\n",
       "      <td>1.630529</td>\n",
       "      <td>0.431525</td>\n",
       "      <td>0.128368</td>\n",
       "      <td>0.106470</td>\n",
       "      <td>0.173876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.470400</td>\n",
       "      <td>1.761135</td>\n",
       "      <td>0.403101</td>\n",
       "      <td>0.097105</td>\n",
       "      <td>0.084075</td>\n",
       "      <td>0.131315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.438400</td>\n",
       "      <td>1.632029</td>\n",
       "      <td>0.453058</td>\n",
       "      <td>0.127804</td>\n",
       "      <td>0.103008</td>\n",
       "      <td>0.168891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.440900</td>\n",
       "      <td>1.576402</td>\n",
       "      <td>0.445306</td>\n",
       "      <td>0.127008</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>0.168435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.459300</td>\n",
       "      <td>1.521934</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>0.158763</td>\n",
       "      <td>0.273272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.395600</td>\n",
       "      <td>1.536645</td>\n",
       "      <td>0.487511</td>\n",
       "      <td>0.239452</td>\n",
       "      <td>0.208648</td>\n",
       "      <td>0.299550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.352400</td>\n",
       "      <td>1.483737</td>\n",
       "      <td>0.486649</td>\n",
       "      <td>0.236235</td>\n",
       "      <td>0.203103</td>\n",
       "      <td>0.296834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.249200</td>\n",
       "      <td>1.521949</td>\n",
       "      <td>0.485788</td>\n",
       "      <td>0.211423</td>\n",
       "      <td>0.171198</td>\n",
       "      <td>0.284074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.369700</td>\n",
       "      <td>1.418759</td>\n",
       "      <td>0.507321</td>\n",
       "      <td>0.235930</td>\n",
       "      <td>0.198396</td>\n",
       "      <td>0.294976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.292200</td>\n",
       "      <td>1.414301</td>\n",
       "      <td>0.505599</td>\n",
       "      <td>0.231142</td>\n",
       "      <td>0.191737</td>\n",
       "      <td>0.293746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.243100</td>\n",
       "      <td>1.435034</td>\n",
       "      <td>0.512489</td>\n",
       "      <td>0.285686</td>\n",
       "      <td>0.254145</td>\n",
       "      <td>0.338964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.212900</td>\n",
       "      <td>1.449587</td>\n",
       "      <td>0.526270</td>\n",
       "      <td>0.286048</td>\n",
       "      <td>0.317043</td>\n",
       "      <td>0.315073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.175700</td>\n",
       "      <td>1.533441</td>\n",
       "      <td>0.516796</td>\n",
       "      <td>0.299436</td>\n",
       "      <td>0.284986</td>\n",
       "      <td>0.338822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.234200</td>\n",
       "      <td>1.476926</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.285794</td>\n",
       "      <td>0.250739</td>\n",
       "      <td>0.343868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.183800</td>\n",
       "      <td>1.421064</td>\n",
       "      <td>0.552972</td>\n",
       "      <td>0.320761</td>\n",
       "      <td>0.297465</td>\n",
       "      <td>0.354828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.134300</td>\n",
       "      <td>1.432725</td>\n",
       "      <td>0.544358</td>\n",
       "      <td>0.326771</td>\n",
       "      <td>0.297991</td>\n",
       "      <td>0.370374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.117900</td>\n",
       "      <td>1.414874</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.326473</td>\n",
       "      <td>0.311221</td>\n",
       "      <td>0.360959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.102300</td>\n",
       "      <td>1.440541</td>\n",
       "      <td>0.557278</td>\n",
       "      <td>0.330814</td>\n",
       "      <td>0.326905</td>\n",
       "      <td>0.350648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.124500</td>\n",
       "      <td>1.477159</td>\n",
       "      <td>0.557278</td>\n",
       "      <td>0.328286</td>\n",
       "      <td>0.313738</td>\n",
       "      <td>0.360976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.160900</td>\n",
       "      <td>1.372655</td>\n",
       "      <td>0.552110</td>\n",
       "      <td>0.333118</td>\n",
       "      <td>0.318227</td>\n",
       "      <td>0.365222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.049800</td>\n",
       "      <td>1.483176</td>\n",
       "      <td>0.529716</td>\n",
       "      <td>0.301998</td>\n",
       "      <td>0.304807</td>\n",
       "      <td>0.339981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.080700</td>\n",
       "      <td>1.418770</td>\n",
       "      <td>0.549526</td>\n",
       "      <td>0.338666</td>\n",
       "      <td>0.324991</td>\n",
       "      <td>0.368175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.032600</td>\n",
       "      <td>1.477586</td>\n",
       "      <td>0.522825</td>\n",
       "      <td>0.331363</td>\n",
       "      <td>0.310684</td>\n",
       "      <td>0.373145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>1.517234</td>\n",
       "      <td>0.527993</td>\n",
       "      <td>0.319314</td>\n",
       "      <td>0.297007</td>\n",
       "      <td>0.364988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>1.487617</td>\n",
       "      <td>0.520241</td>\n",
       "      <td>0.323958</td>\n",
       "      <td>0.328752</td>\n",
       "      <td>0.347538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>1.473606</td>\n",
       "      <td>0.543497</td>\n",
       "      <td>0.332077</td>\n",
       "      <td>0.312047</td>\n",
       "      <td>0.368667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.939600</td>\n",
       "      <td>1.421801</td>\n",
       "      <td>0.569337</td>\n",
       "      <td>0.353240</td>\n",
       "      <td>0.337563</td>\n",
       "      <td>0.379537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.008200</td>\n",
       "      <td>1.445441</td>\n",
       "      <td>0.560724</td>\n",
       "      <td>0.346171</td>\n",
       "      <td>0.330116</td>\n",
       "      <td>0.370069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.990800</td>\n",
       "      <td>1.440470</td>\n",
       "      <td>0.540052</td>\n",
       "      <td>0.351214</td>\n",
       "      <td>0.333375</td>\n",
       "      <td>0.380202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.875700</td>\n",
       "      <td>1.577077</td>\n",
       "      <td>0.553833</td>\n",
       "      <td>0.339253</td>\n",
       "      <td>0.333341</td>\n",
       "      <td>0.361713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>1.526615</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.357641</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.383713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>1.474417</td>\n",
       "      <td>0.564169</td>\n",
       "      <td>0.347027</td>\n",
       "      <td>0.329118</td>\n",
       "      <td>0.374580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>1.436659</td>\n",
       "      <td>0.530577</td>\n",
       "      <td>0.352384</td>\n",
       "      <td>0.334052</td>\n",
       "      <td>0.380386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.775600</td>\n",
       "      <td>1.584827</td>\n",
       "      <td>0.525409</td>\n",
       "      <td>0.350324</td>\n",
       "      <td>0.364920</td>\n",
       "      <td>0.366451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.946600</td>\n",
       "      <td>1.513138</td>\n",
       "      <td>0.527132</td>\n",
       "      <td>0.345143</td>\n",
       "      <td>0.362394</td>\n",
       "      <td>0.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.900400</td>\n",
       "      <td>1.461163</td>\n",
       "      <td>0.513351</td>\n",
       "      <td>0.364208</td>\n",
       "      <td>0.347186</td>\n",
       "      <td>0.387141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.847400</td>\n",
       "      <td>1.508899</td>\n",
       "      <td>0.544358</td>\n",
       "      <td>0.361142</td>\n",
       "      <td>0.356441</td>\n",
       "      <td>0.376708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>1.494516</td>\n",
       "      <td>0.530577</td>\n",
       "      <td>0.376206</td>\n",
       "      <td>0.359222</td>\n",
       "      <td>0.402114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.813600</td>\n",
       "      <td>1.522929</td>\n",
       "      <td>0.516796</td>\n",
       "      <td>0.379514</td>\n",
       "      <td>0.362442</td>\n",
       "      <td>0.408336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>1.456708</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.382296</td>\n",
       "      <td>0.371355</td>\n",
       "      <td>0.402403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>1.476713</td>\n",
       "      <td>0.539190</td>\n",
       "      <td>0.361640</td>\n",
       "      <td>0.356789</td>\n",
       "      <td>0.374253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.750900</td>\n",
       "      <td>1.562336</td>\n",
       "      <td>0.552110</td>\n",
       "      <td>0.380621</td>\n",
       "      <td>0.375961</td>\n",
       "      <td>0.389185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.846500</td>\n",
       "      <td>1.535362</td>\n",
       "      <td>0.534022</td>\n",
       "      <td>0.351939</td>\n",
       "      <td>0.359685</td>\n",
       "      <td>0.353774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.849700</td>\n",
       "      <td>1.498003</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.377856</td>\n",
       "      <td>0.377552</td>\n",
       "      <td>0.382737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>1.459401</td>\n",
       "      <td>0.554694</td>\n",
       "      <td>0.372085</td>\n",
       "      <td>0.361681</td>\n",
       "      <td>0.391758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>1.571154</td>\n",
       "      <td>0.540052</td>\n",
       "      <td>0.369605</td>\n",
       "      <td>0.358253</td>\n",
       "      <td>0.389327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>1.588504</td>\n",
       "      <td>0.535745</td>\n",
       "      <td>0.378004</td>\n",
       "      <td>0.369375</td>\n",
       "      <td>0.393293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>1.578725</td>\n",
       "      <td>0.530577</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.365401</td>\n",
       "      <td>0.408740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.743700</td>\n",
       "      <td>1.536840</td>\n",
       "      <td>0.524548</td>\n",
       "      <td>0.381536</td>\n",
       "      <td>0.363795</td>\n",
       "      <td>0.404917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>1.586156</td>\n",
       "      <td>0.517657</td>\n",
       "      <td>0.385059</td>\n",
       "      <td>0.374198</td>\n",
       "      <td>0.404312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>1.619803</td>\n",
       "      <td>0.521964</td>\n",
       "      <td>0.369942</td>\n",
       "      <td>0.360780</td>\n",
       "      <td>0.398784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>1.565600</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.382164</td>\n",
       "      <td>0.376661</td>\n",
       "      <td>0.397808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>1.635299</td>\n",
       "      <td>0.521102</td>\n",
       "      <td>0.380656</td>\n",
       "      <td>0.377164</td>\n",
       "      <td>0.391942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>1.665520</td>\n",
       "      <td>0.519380</td>\n",
       "      <td>0.381483</td>\n",
       "      <td>0.374517</td>\n",
       "      <td>0.402157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>1.607974</td>\n",
       "      <td>0.513351</td>\n",
       "      <td>0.384102</td>\n",
       "      <td>0.370685</td>\n",
       "      <td>0.407177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.713700</td>\n",
       "      <td>1.631065</td>\n",
       "      <td>0.500431</td>\n",
       "      <td>0.378260</td>\n",
       "      <td>0.372274</td>\n",
       "      <td>0.395273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.770800</td>\n",
       "      <td>1.636394</td>\n",
       "      <td>0.506460</td>\n",
       "      <td>0.387135</td>\n",
       "      <td>0.375913</td>\n",
       "      <td>0.410996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.640200</td>\n",
       "      <td>1.646958</td>\n",
       "      <td>0.527993</td>\n",
       "      <td>0.387752</td>\n",
       "      <td>0.375112</td>\n",
       "      <td>0.407317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.713600</td>\n",
       "      <td>1.616310</td>\n",
       "      <td>0.522825</td>\n",
       "      <td>0.389188</td>\n",
       "      <td>0.377941</td>\n",
       "      <td>0.409479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>1.623837</td>\n",
       "      <td>0.515073</td>\n",
       "      <td>0.384519</td>\n",
       "      <td>0.376619</td>\n",
       "      <td>0.405692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>1.661279</td>\n",
       "      <td>0.501292</td>\n",
       "      <td>0.379981</td>\n",
       "      <td>0.378118</td>\n",
       "      <td>0.398292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.647600</td>\n",
       "      <td>1.653839</td>\n",
       "      <td>0.494401</td>\n",
       "      <td>0.384938</td>\n",
       "      <td>0.384068</td>\n",
       "      <td>0.391043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.608800</td>\n",
       "      <td>1.708597</td>\n",
       "      <td>0.484065</td>\n",
       "      <td>0.392547</td>\n",
       "      <td>0.392788</td>\n",
       "      <td>0.406443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.628500</td>\n",
       "      <td>1.701717</td>\n",
       "      <td>0.500431</td>\n",
       "      <td>0.381463</td>\n",
       "      <td>0.378212</td>\n",
       "      <td>0.395181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>1.713157</td>\n",
       "      <td>0.507321</td>\n",
       "      <td>0.375235</td>\n",
       "      <td>0.376274</td>\n",
       "      <td>0.384099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.607500</td>\n",
       "      <td>1.710165</td>\n",
       "      <td>0.491817</td>\n",
       "      <td>0.390684</td>\n",
       "      <td>0.387997</td>\n",
       "      <td>0.403130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>1.700400</td>\n",
       "      <td>0.521102</td>\n",
       "      <td>0.390074</td>\n",
       "      <td>0.391648</td>\n",
       "      <td>0.404355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.617400</td>\n",
       "      <td>1.720513</td>\n",
       "      <td>0.499569</td>\n",
       "      <td>0.380914</td>\n",
       "      <td>0.388346</td>\n",
       "      <td>0.392714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>1.734169</td>\n",
       "      <td>0.494401</td>\n",
       "      <td>0.383430</td>\n",
       "      <td>0.380628</td>\n",
       "      <td>0.392474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>1.766576</td>\n",
       "      <td>0.472007</td>\n",
       "      <td>0.378448</td>\n",
       "      <td>0.371717</td>\n",
       "      <td>0.396683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>1.750567</td>\n",
       "      <td>0.506460</td>\n",
       "      <td>0.381869</td>\n",
       "      <td>0.378179</td>\n",
       "      <td>0.394924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>1.756064</td>\n",
       "      <td>0.487511</td>\n",
       "      <td>0.386108</td>\n",
       "      <td>0.383320</td>\n",
       "      <td>0.398226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.600700</td>\n",
       "      <td>1.763385</td>\n",
       "      <td>0.480620</td>\n",
       "      <td>0.386841</td>\n",
       "      <td>0.386281</td>\n",
       "      <td>0.398047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.583600</td>\n",
       "      <td>1.760033</td>\n",
       "      <td>0.480620</td>\n",
       "      <td>0.384074</td>\n",
       "      <td>0.386217</td>\n",
       "      <td>0.394288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>1.766754</td>\n",
       "      <td>0.480620</td>\n",
       "      <td>0.383486</td>\n",
       "      <td>0.381478</td>\n",
       "      <td>0.395362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>1.749690</td>\n",
       "      <td>0.490956</td>\n",
       "      <td>0.390499</td>\n",
       "      <td>0.385849</td>\n",
       "      <td>0.403885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>1.760869</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.390103</td>\n",
       "      <td>0.386929</td>\n",
       "      <td>0.401666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>1.779761</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.384979</td>\n",
       "      <td>0.385731</td>\n",
       "      <td>0.394892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.546800</td>\n",
       "      <td>1.783244</td>\n",
       "      <td>0.479759</td>\n",
       "      <td>0.383078</td>\n",
       "      <td>0.384748</td>\n",
       "      <td>0.391386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.604300</td>\n",
       "      <td>1.782738</td>\n",
       "      <td>0.476314</td>\n",
       "      <td>0.379926</td>\n",
       "      <td>0.382215</td>\n",
       "      <td>0.388062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./TTC4900Model/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./TTC4900Model/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4360, training_loss=0.948503929650018, metrics={'train_runtime': 1401.4827, 'train_samples_per_second': 49.676, 'train_steps_per_second': 3.111, 'total_flos': 8658485142130560.0, 'train_loss': 0.948503929650018, 'epoch': 20.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3972e162-4638-4411-8144-4c014046f35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_Accuracy</th>\n",
       "      <th>eval_F1</th>\n",
       "      <th>eval_Precision</th>\n",
       "      <th>eval_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.837214</td>\n",
       "      <td>0.723355</td>\n",
       "      <td>0.470469</td>\n",
       "      <td>0.438131</td>\n",
       "      <td>0.519986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1.436659</td>\n",
       "      <td>0.530577</td>\n",
       "      <td>0.352384</td>\n",
       "      <td>0.334052</td>\n",
       "      <td>0.380386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>1.454459</td>\n",
       "      <td>0.551249</td>\n",
       "      <td>0.362453</td>\n",
       "      <td>0.354084</td>\n",
       "      <td>0.387909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eval_loss  eval_Accuracy   eval_F1  eval_Precision  eval_Recall\n",
       "train   0.837214       0.723355  0.470469        0.438131     0.519986\n",
       "val     1.436659       0.530577  0.352384        0.334052     0.380386\n",
       "test    1.454459       0.551249  0.362453        0.354084     0.387909"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [trainer.evaluate(eval_dataset=dataset) for dataset in [train_dataloader, val_dataloader, test_dataset]]\n",
    "\n",
    "pd.DataFrame(q, index=[\"train\", \"val\",\"test\"]).iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "77744ed6-2f2e-42bb-b1dc-812f554ed742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc4fa1e5-57e8-4305-9998-f340eb4acfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Get model output (logits)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred_label_idx = probs.argmax()\n",
    "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
    "\n",
    "    return probs, pred_label_idx, pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0d392f0f-3b0c-400a-b273-246b9e8ad75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Максим Олегович написал на доске 2016 целых чисел. Робот Крякен заметил, что сумма любых 2015 чисел четна. Четна или нечетна сумма всех чисел?  Источник: https://shkolkovo.net/catalog/zadachi_na_teoriyu_chisel/chetnost_i_nechetnost/page-2 © shkolkovo.net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2.4956e-04, 1.3027e-02, 8.6578e-03, 1.9922e-02, 2.9243e-02, 5.1015e-03,\n",
       "          9.2223e-01, 1.5650e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor(6, device='cuda:0'),\n",
       " 'Теория чисел')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with a an example text in Turkish\n",
    "text = input()\n",
    "# \"Machine Learning itself is moving towards more and more automated\"\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "379b2251-de39-4406-9cf0-2930a45dd347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Геометрия',\n",
       " 1: 'Дирихле',\n",
       " 2: 'Инвариант',\n",
       " 3: 'Многочлен',\n",
       " 4: 'Комбинаторика',\n",
       " 5: 'Оценка+Пример',\n",
       " 6: 'Теория чисел',\n",
       " 7: 'Графы'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a11dce16-95fb-44d2-a9b3-7e64e2697ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1\n",
       "y      \n",
       "0   256\n",
       "1   305\n",
       "2   145\n",
       "3   262\n",
       "4   657\n",
       "5   142\n",
       "6  1493\n",
       "7   221"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = y_train.to_frame()\n",
    "df['1'] = 1\n",
    "df.groupby(by=['y']).agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc44d8-0227-4a23-a38f-38759a0b7784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
