{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a969c31-289c-451d-aaee-85835a0eafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8128fd9a-c641-4ce5-bde7-416cf54c4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from yargy import Parser, rule, and_, or_\n",
    "from yargy.predicates import gram\n",
    "from yargy.interpretation import fact, attribute\n",
    "from yargy.predicates import normalized, dictionary\n",
    "from yargy.pipelines import morph_pipeline\n",
    "from yargy.relations import main\n",
    "from yargy.predicates import in_\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pymorphy2\n",
    "from gensim.models import Word2Vec as w2v\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24819df5-fd99-4188-98e9-0b52c99dd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords = [\n",
    "    'окружность', 'высота', 'треугольникabc', 'хорда', 'четырехугольник',\n",
    "    'точка', 'касательная', 'угол', 'пересечение', 'прямая', 'середина',\n",
    "    'центр', 'треугольник', 'пересекаться', 'лежать', 'проходить',\n",
    "    'различный', 'закрасить', 'делиться', 'сумма', 'существует',\n",
    "    'больший', 'известный', 'клетка', 'каждый', 'цвет', 'доска',\n",
    "    'малый', 'таблица', 'выбрать', 'найдется', 'фишка',\n",
    "    'добиться', 'стоить', 'получить',\n",
    "    'операция', 'написать', 'соседний',\n",
    "    'количество', 'оказаться', 'стакан','ход',\n",
    "    'разрешаться', 'многочлен', 'функция', 'уравнение', 'коэффициент',\n",
    "    'кубический', 'неравенство', 'равенство', 'член',\n",
    "    'трехчлен', 'равный', 'известный', 'график', 'целый',\n",
    "    'положительный', 'действительный', 'сумма', 'натуральный',\n",
    "    'делиться', 'существовать', 'произведение'\n",
    "    'неравенство', 'группа', 'квадратный', 'команда',\n",
    "    'положительный', 'неизвестная', 'разделить', 'многочлен',\n",
    "    'коэффициент', 'расставить', 'число', 'клетка', 'функция',\n",
    "    'цвет', 'составить', 'член', 'буква', 'равный', 'произведение',\n",
    "    'сумма', 'соседний', 'состоять', 'известный', 'ровно', 'получить',\n",
    "    'количество', 'равенство', 'целый', 'различный', 'порядок',\n",
    "    'делиться', 'график', 'натуральный', 'уравнение', 'существовать',\n",
    "    'человек', 'действительный', 'город', 'соединить', 'трехчлен',\n",
    "    'корень', 'множители', 'способ', 'сколько',\n",
    "    \"сумма\", \"каждый\", \"коробка\",\n",
    "    \"малый\", \"белый\",\"больший\", \"набор\",\n",
    "    \"бить\", \"отметить\", \"больший количество\",\n",
    "    \"больший\", \"шахматный\",\n",
    "    \"клетка\", 'квадрат', 'остальные', 'доска',\n",
    "    'число', 'сумма', 'квадрат', 'оказаться',\n",
    "    'черный', 'отметить', 'остаться',\n",
    "    'набор', 'записать', 'возможный', 'какой',\n",
    "    'какой малый', 'какой больший', 'больший количество',\n",
    "    'шахматный доска', 'возможный', 'малый возможный',\n",
    "    'найти малый', 'какой минимальный', 'минимальный',\n",
    "    'максимальный', 'какой максимальный', 'последовательность',\n",
    "    'получить', 'король', 'размер', 'клетка'\n",
    "    'некоторый', 'решить', 'сложение', 'порядок',\n",
    "    'нечетный', 'цифра', 'оканчивается', 'четный',\n",
    "    'целый', 'число делиться', 'простой', 'натуральный число',\n",
    "    'делится', 'целый число', 'mod', 'простой число',\n",
    "    'движение', 'город соединить', 'проехать',\n",
    "    'связанный', 'каждый город', 'вершина',\n",
    "    'добраться', 'дорога', 'страна', 'команда',\n",
    "    'человек', 'граф', 'соединить', 'город', 'знакомый', 'провод'\n",
    "    \n",
    "] \n",
    "Case = fact('Case', ['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcac74ec-5525-469e-a94c-1db16b5a4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_one_to_one(topic: list):\n",
    "    \"\"\"\n",
    "    Функция для формирования pipline слова\n",
    "    \"\"\"\n",
    "    return morph_pipeline(topic).interpretation(\n",
    "          Case.name.const(topic[0])\n",
    "      ).interpretation(\n",
    "          Case\n",
    "      )\n",
    "\n",
    "def add_keywords(df, keywords, column_name):\n",
    "    \"\"\"\n",
    "    Функция, которая добавляет ключевые слова в dataset\n",
    "    \"\"\"\n",
    "    alls_word_pipe = []\n",
    "    for word in keywords:\n",
    "        alls_word_pipe.append(make_topic_one_to_one([word]))\n",
    "        df[word] = np.NaN\n",
    "\n",
    "    ALL = or_(*alls_word_pipe).interpretation(Case)\n",
    "    parser = Parser(ALL)\n",
    "\n",
    "    for ind, elem in enumerate(tqdm(df[column_name])):\n",
    "        for match in parser.findall(str(elem)):\n",
    "            df.loc[ind, match.fact] = 1\n",
    "    df.fillna(0, inplace=True)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def add_keywords(df, keywords, column_name):\n",
    "    \"\"\"\n",
    "    Функция, которая добавляет ключевые слова в dataset\n",
    "    \"\"\"\n",
    "    alls_word_pipe = []\n",
    "    for word in keywords:\n",
    "        alls_word_pipe.append(make_topic_one_to_one([word]))\n",
    "        df[word] = np.NaN\n",
    "\n",
    "    ALL = or_(*alls_word_pipe).interpretation(Case)\n",
    "    parser = Parser(ALL)\n",
    "\n",
    "    for ind, elem in enumerate(tqdm(df[column_name])):\n",
    "        for match in parser.findall(str(elem)):\n",
    "            df.loc[ind, match.fact] = 1\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "with open('name-to-name.pickle', 'rb') as file:\n",
    "    name_to_name_dict = pickle.load(file)\n",
    "\n",
    "\n",
    "# Чистим ненужные символы\n",
    "def replace_symbol(text):\n",
    "    text = re.sub(r'[\\d′‖√∠ω№◦⊥→×]', lambda match: name_to_name_dict[match.group()], text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def del_garbage(text):\n",
    "    return text.strip('.,)?.<*&^%$#@!~')\\\n",
    "               .replace(u'\\u00A0', ' ')\\\n",
    "               .replace(u'\\x0c', ' ')\\\n",
    "               .replace('\\n', '').replace('\\t', '')\\\n",
    "               .replace('\\r', '')\n",
    "\n",
    "\n",
    "# Убираем пунтктуацию\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "# Лемматизируем текст\n",
    "def lemmatize(doc, stopwords=stopwords_ru):\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Финальная функция обработки текста\n",
    "def preprocessing_text(text):\n",
    "    text = del_garbage(text).lower()\n",
    "    text = lemmatize(text)\n",
    "    text = replace_symbol(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def vect_tfidf(vectorizer, text):\n",
    "    \"\"\"\n",
    "    Функция, которая добавляет векторное представлние текста с помощью модели vectorizer\n",
    "    \"\"\"\n",
    "    return vectorizer.transform([text]).toarray()\n",
    "\n",
    "\n",
    "def add_tf_idf(data, X_columns='new_task'):\n",
    "    \"\"\"\n",
    "    Функция, которая векторизует датасет c помощью tf-idf\n",
    "    \"\"\"\n",
    "    vectorizer = read_models(\"vectorizer.pickle\")\n",
    "    train_title_tfidf = np.vstack(data[X_columns].apply(lambda x: vect_tfidf(vectorizer, x)))\n",
    "    for i in range(len(vectorizer.get_feature_names_out())):\n",
    "        data[f\"Tiidf title f.{i + 1}\"] = train_title_tfidf[:, i]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_embedings(data, X_columns='new_task'):\n",
    "    \"\"\"\n",
    "    Функция, которая векторизует датасет c помощью word2vec\n",
    "    \"\"\"\n",
    "    model = w2v.load('word2vec_math_text.model')\n",
    "    for i in range(100):\n",
    "      data[f'vector_{i}'] = 0\n",
    "\n",
    "    for j, text in enumerate(train[X_columns]):\n",
    "      vec = np.zeros(100)\n",
    "      lens = 0\n",
    "      for word in word_tokenize(text):\n",
    "          try:\n",
    "            vec += model.wv[word]\n",
    "            lens += 1\n",
    "          except KeyError:\n",
    "            continue\n",
    "    vec /= lens\n",
    "    for i in range(100):\n",
    "        data.iloc[j, data.shape[1]-100+i] = vec[i]\n",
    "    return data\n",
    "\n",
    "def add_tokens(data, X_columns='new_task'):\n",
    "    \"\"\"\n",
    "    Функция, которая векторизирует dataset с помощью tensorflow\n",
    "    \"\"\"\n",
    "    vocab_size = 20000\n",
    "    trunc_type = 'post'\n",
    "    padding_type = 'post'\n",
    "    embedding_dim = 128\n",
    "    max_length = 120\n",
    "    oov_tok = '<OOV>'\n",
    "    \n",
    "    tokenizer = read_models(\"tokenizer.pickle\")\n",
    "    \n",
    "    train_sequences = tokenizer.texts_to_sequences(train[X_columns])\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_length,\n",
    "                                 padding=padding_type, truncating=trunc_type)\n",
    "    for i in range(max_length):\n",
    "        data[f\"Tokens f.{i + 1}\"] = train_padded[:, i]\n",
    "    train.fillna(0, inplace=True)\n",
    "    try:\n",
    "      data.drop(columns=[None], inplace=True)\n",
    "    except KeyError as err:\n",
    "        pass\n",
    "\n",
    "\n",
    "def read_models(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f74e5f9-2ad6-4e79-9625-42254a4ef30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessing_data_big.csv', sep='@').drop(columns=['Unnamed: 0', 'index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73771249-fc75-4003-901c-95a51c1bc55c",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a46fd56-0b6f-48b4-a4b2-a10b5af0b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5803/5803 [26:46<00:00,  3.61it/s]  \n"
     ]
    }
   ],
   "source": [
    "data_with_keywords = add_keywords(df=data, keywords=Keywords, column_name='task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c4e52c5-70b9-4fac-adb6-693192166d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_keywords.to_csv('data_with_keywords.csv', sep='@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbd7eb-0f8e-43bc-9ff7-ea30235ec729",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_with_keywords.copy(deep=True)\n",
    "data = add_tf_idf(train, X_columns='new_task')\n",
    "data = add_tokens(train, X_columns='new_task')\n",
    "data = add_embedings(train, X_columns='new_task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2679b-f1be-4f29-9265-93a3b1c595bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['task', 'new_task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e9853-79a7-4228-8b92-6d10e7c8ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### class StackingModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.final_models = XGBClassifier()\n",
    "        self.models_fit = [(Ridger, X_test.iloc[:, 100:100+4146]),\n",
    "                           (LogisticRegression, X_test.iloc[:, :4146]),\n",
    "                           (SGDr, X_test.iloc[:, :4146]),\n",
    "                           (SVC, X_test.iloc[:, 100:100+4146]),\n",
    "                           (KNN, X_test.iloc[:, 100:100+4146]),\n",
    "                           (RandomForest, X_test.iloc[:, 100:100+4146]),\n",
    "                           (XGB, X_test.iloc[:, 100:100+4146]),\n",
    "                           (CatBoost, X_test.iloc[:, 100:100+4146])]\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_train, tr_2, tr_3, X_val, y_train, y_val = train_val_split(X, y)\n",
    "        val_2 = X_val.iloc[:, 114].to_frame()\n",
    "        val_3 = X_val.iloc[:, 115:]\n",
    "        for ind, model in enumerate(self.models):\n",
    "            if ind == 0 or ind == 3:\n",
    "              model.fit(tr_3, y_train)\n",
    "            elif ind == 4:\n",
    "              model.fit(tr_2, y_train)\n",
    "            else:\n",
    "              model.fit(X_train, y_train)\n",
    "\n",
    "        print('Models trained')\n",
    "        stack_df = self.transform(X_val)\n",
    "        self.regressor_final.fit(stack_df, y_val)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        stack_df = pd.DataFrame({'lg': self.models[0].predict(X.iloc[:, 115:]),\n",
    "                                'rig': self.models[1].predict(X),\n",
    "                                'lasso': self.models[2].predict(X),\n",
    "                                'bayes': self.models[3].predict(X.iloc[:, 115:]),\n",
    "                                'svr': self.models[4].predict(X.iloc[:, 114].to_frame()),\n",
    "                                'knn': self.models[5].predict(X),\n",
    "                                'free': self.models[6].predict(X),\n",
    "                                'fr': self.models[7].predict(X),\n",
    "                                'grboost': self.models[8].predict(X),\n",
    "                                'xgboost':self.models[9].predict(X),\n",
    "                                'catboost': self.models[10].predict(X),\n",
    "                                'MolLogP': list(X.MolLogP)})\n",
    "        return stack_df\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        stack_df = self.transform(X)\n",
    "        return self.regressor_final.predict(stack_df)\n",
    "\n",
    "\n",
    "    def train_val_split(X, y):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "    \n",
    "        tr_2 = X_train.iloc[:, 114].to_frame()\n",
    "        tr_3 = X_train.iloc[:, 115:]\n",
    "    \n",
    "        return X_train, tr_2, tr_3, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "    def number_of_atoms(atom_list, df):\n",
    "        for i in atom_list:\n",
    "            df['num_of_{}_atoms'.format(i)] = df['mol'].apply(lambda x: len(x.GetSubstructMatches(Chem.MolFromSmiles(i))))\n",
    "    \n",
    "    \n",
    "    def ecfc_molstring(molecule, radius=3, size=1024):\n",
    "        arr = np.zeros((1,), dtype=int)\n",
    "        DataStructs.ConvertToNumpyArray(\n",
    "            AllChem.GetHashedMorganFingerprint(molecule, radius, size, useFeatures=False),\n",
    "            arr,\n",
    "        )\n",
    "        return arr\n",
    "    \n",
    "    def mols2features(mols):\n",
    "        return np.array([ecfc_molstring(mol) for mol in mols])\n",
    "    \n",
    "    def CalcDescriptors(data):\n",
    "        df = pd.DataFrame({name: [] for name in descList})\n",
    "        result = np.asarray([calculator.CalcDescriptors(mol) for mol in data.mol]).transpose()\n",
    "        for i, descriptor in enumerate(descList):\n",
    "            df[descriptor] = result[i]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b11ad8-c762-4efd-bd45-1ae6082713a4",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e47d6-5143-4f11-8425-efb3b118f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ml = read_models(\"model_ml\")\n",
    "def get_ml(text):\n",
    "    data = pd.DataFrame({'text': text,\n",
    "                         'proc_text': preprocessing_text(text))\n",
    "    \n",
    "    data = add_keywords(df=data, keywords=Keywords, column_name='text')\n",
    "    data = add_tf_idf(data, X_columns='text')\n",
    "    data = add_tokens(data, X_columns='text')\n",
    "    data = add_embedings(data, X_columns='text')\n",
    "    return model_ml.predict_proba(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
